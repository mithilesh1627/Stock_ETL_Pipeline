# ğŸ“ˆ Stock ETL Pipeline with Airflow, MongoDB & Streamlit

This project demonstrates a modular, production-style **ETL pipeline** for stock market data using:

- **Apache Airflow** for orchestration  
- **MongoDB** for storage  
- **Streamlit** for visualization

---

## ğŸš€ Features

- Modularized ETL design (Extract, Transform, Load)
- DAG-based orchestration via Apache Airflow
- Secrets/config handled via external JSON
- Raw & processed data stored in MongoDB
- Streamlit dashboard for visualization
- Real-world reproducible structure

---

## ğŸ“¦ Tech Stack

- Python 3.10  
- Apache Airflow  
- MongoDB  
- Streamlit  
- JSON (for config)  

---

## ğŸ“ Project Structure

```
stock-etl-pipeline/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_stock_pipeline.py        # Airflow DAG definition
â”‚
â”œâ”€â”€ stock_etl_model/
â”‚   â”œâ”€â”€ extract_stage.py             # Extract stage logic
â”‚   â”œâ”€â”€ transform_stage.py           # Transform stage logic
â”‚   â””â”€â”€ load_stage.py                # Load stage logic
â”‚
â”œâ”€â”€ output/                          # Screenshots of ETL & dashboard
â”‚
â”œâ”€â”€ streamlit_app.py                 # Streamlit visualization
â”œâ”€â”€ password.json                    # Secrets (excluded from Git)
â””â”€â”€ README.md                        # Project description
```

---

## ğŸ” DAG Overview

Workflow in Airflow DAG:

```text
1. get_config â†’ reads secret config
2. extract_config_values â†’ parses necessary keys
3. extract â†’ calls API with credentials
4. transform â†’ processes data
5. load â†’ inserts data into MongoDB
```

---

## ğŸ–¼ï¸ Screenshots

| Airflow DAG | DAG Triggered |
|-------------|----------------|
| ![](output/DAG_graph.PNG) | ![](output/Manual_trigged_DAG.PNG) |

| All Tasks Completed | MongoDB Output |
|---------------------|----------------|
| ![](output/all_task_run_sucessfully.PNG) | ![](output/MongoDB_After_ETL_RUN.PNG) |

| Streamlit Preview |
|-------------------|
| ![](output/StreamLit_After_ETL_Run.PNG) |

---

## ğŸ“Š Streamlit Dashboard

Launch the dashboard locally:

```bash
streamlit run streamlit_app.py
```

---

## ğŸ” Config File Format (`password.json`)

This file is required but should not be pushed to GitHub. Example:

```json
{
  "symbol": "IBM",
  "password": "your_api_key",
  "mongo_uri": "mongodb://localhost:27017",
  "db_name": "stock_data",
  "collection_name": "ibm_data"
}
```

---

## ğŸ§  Learnings

- Designed a real-time ETL flow from scratch  
- Handled Airflow task dependencies using `XCom`  
- Managed configs securely outside of DAG  
- Created an interactive dashboard with Streamlit  
- Simulated a production-ready pipeline  

---

## ğŸ‘¤ Author

**Mithilesh Chaurasiya**    
ğŸ”— [Portfolio](https://mithileshcv.up.railway.app)  
ğŸ”— [LinkedIn](https://linkedin.com/in/mithilesh1627)

---

## ğŸŒŸ Star the Repo

If this project helped you understand real-world Airflow-based pipelines, consider giving it a â­ on GitHub!
